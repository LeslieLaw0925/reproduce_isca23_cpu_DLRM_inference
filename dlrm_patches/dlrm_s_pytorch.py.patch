diff --git a/models/recommendation/pytorch/dlrm/product/dlrm_s_pytorch.py b/models/recommendation/pytorch/dlrm/product/dlrm_s_pytorch.py
index 3abfeedd..85064ca3 100644
--- a/models/recommendation/pytorch/dlrm/product/dlrm_s_pytorch.py
+++ b/models/recommendation/pytorch/dlrm/product/dlrm_s_pytorch.py
@@ -459,6 +459,7 @@ def trace_model(args, dlrm, test_ld):
 def run_throughput_benchmark(args, dlrm, test_ld):
     if args.num_cpu_cores != 0:
         torch.set_num_threads(1)
+    ipex._C.start_embed_prof()
     bench = ThroughputBenchmark(dlrm)
     for j, inputBatch in enumerate(test_ld):
         X, lS_o, lS_i, T, W, CBPP = unpack_batch(inputBatch)
@@ -468,7 +469,7 @@ def run_throughput_benchmark(args, dlrm, test_ld):
     stats = bench.benchmark(
         num_calling_threads=args.share_weight_instance,
         num_warmup_iters=100,
-        num_iters=args.num_batches * args.share_weight_instance,
+        num_iters=args.num_batches,
     )
     print(stats)
     latency = stats.latency_avg_ms
@@ -675,6 +676,13 @@ def run():
                         The Terabyte dataset can be multiprocessed in an environment \
                         with more than 24 CPU cores and at least 1 TB of memory.",
     )
+    parser.add_argument("--data-generation", type=str, default="random")
+    parser.add_argument("--data-trace-file", type=str, default="./input/dist_emb_j.log")
+    parser.add_argument("--data-trace-enable-padding", type=bool, default=False)
+    parser.add_argument("--num-workers", type=int, default=0)
+    parser.add_argument("--data-size", type=int, default=1)
+    parser.add_argument("--num-indices-per-lookup", type=int, default=10)
+    parser.add_argument("--num-indices-per-lookup-fixed", type=bool, default=False)
     # training
     parser.add_argument("--mini-batch-size", type=int, default=1)
     parser.add_argument("--nepochs", type=int, default=1)
@@ -740,6 +748,7 @@ def run():
     ln_bot = np.fromstring(args.arch_mlp_bot, dtype=int, sep="-")
     # input data
 
+    '''
     train_data, train_ld, test_data, test_ld = dp.make_criteo_data_and_loaders(args)
     nbatches = args.num_batches if args.num_batches > 0 else len(train_ld)
     nbatches_test = len(test_ld)
@@ -759,6 +768,13 @@ def run():
         ln_emb = np.array(ln_emb)
     m_den = train_data.m_den
     ln_bot[0] = m_den
+    '''
+    ln_emb = np.fromstring(args.arch_embedding_size, dtype=int, sep="-")
+    m_den = ln_bot[0]
+    train_data, train_ld = dp.make_random_data_and_loader(args, ln_emb, m_den)
+    test_data, test_ld = dp.make_random_data_and_loader(args, ln_emb, m_den)
+    nbatches = args.num_batches if args.num_batches > 0 else len(train_ld)
+    nbatches_test = len(test_ld)
 
     args.ln_emb = ln_emb.tolist()
 
